# Tools for GigaDB: dataset-backup-tool

## Set up

To run the Tencent backup smoke tests, the tests need access to a Tencent Cloud
account which is provided by these variables: `TENCENTCLOUD_SECRET_ID`,
`TENCENTCLOUD_SECRET_KEY`, `TENCENTCLOUD_APP_ID`. These variables need to be 
added as new GitLab secrets. 

These new variables need to be pulled into the `.secrets` file and a 
configuration file, `.cos.conf` and shell scripts, `create_bucket.sh` and `
delete_bucket.sh` created in the `dataset-backup-tool/scripts` directory:
```
$ docker-compose run --rm config
```
The smoke tests uses these shell scripts for creating and deleting a Tencent 
bucket at the start and end of the tests. The `.cos.conf` file provides 
configuration for running `coscmd` commands in `BackupSmokeCest` functional test 
class.

## Run Tencent coscmd based backup smoke tests

Change directory to the `dataset-backup-tool`:
```
$ cd gigadb/app/tools/dataset-backup-tool
```

There are 3 smoke tests in `tests/functional/BackupSmokeCest` which backup data
files to a `dataset` directory in a Tencent bucket:
* `tryBackupDataset` will upload 3 files from the `tests/_data/dataset1` 
  directory into the `dataset` directory in a Tencent bucket.
* `tryUpdateBackupWithChangedFile` checks that the `coscmd` tool can detect 
  differences between files. This test should only upload `test.csv` from
  `tests/_data/dataset2` into the Tencent `dataset` backup directory since only 
  this csv file is different in the `dataset2` directory compared to the 
  `dataset1` directory.
* `tryUpdateBackupWithDeletedFile` checks that the `--delete` parameter is able
  to synchronise the contents of a source directory with its counterpart 
  directory in a Tencent bucket. The `tests/_data/dataset3` directory is missing
  `test.tsv` so this test checks that the `test.tsv` file in the Tencent bucket 
  `dataset` directory has been deleted.

To run these smoke tests:
```
$ docker-compose run --rm backup_tool ./vendor/bin/codecept run tests/functional/BackupSmokeCest.php
```

## RClone

### Setup

Rclone is the recommended approach for operating the backup workflows.
``gigadb/app/tools/dataset-backup-tool/config/rclone.conf`` is the configuration file to enable rclone to operate on Tencent Cloud.
That configuration is generated by the same ``generate_config.sh`` aforementioned
based on the template ``gigadb/app/tools/dataset-backup-tool/config/rclone.conf.dist``

The setup has its own smoke tests that can be run using this way:
```
$ cd gigadb/app/tools/dataset-backup-tool
$ docker-compose run --rm backup_tool ./vendor/bin/codecept run -g rclone-setup tests/functional
```

For the benefit of the tests, the configuration file is bind-mounted to the ``backup_tool`` 
container service in ``gigadb/app/tools/dataset-backup-tool/docker-compose.yml`` and rclone itself is deployed to the 
base image in ``gigadb/app/tools/dataset-backup-tool/Dockerfile``.

### Scripts

In order to reduce human error by:
 * making dry-run mode the default mode
 * forcing use of --verbose (because rclone shows no logging at all otherwise)  
 * asking confirmation before deleting something
 * reducing opportunities for operator to make typo or use wrong values

we need to:

 * wrap the ``rclone`` commands within thin wrapper scripts:
    * ``gigadb/app/tools/dataset-backup-tool/scripts/sync_files.sh`` for the incremental backup
    * ``gigadb/app/tools/dataset-backup-tool/scripts/delete_files.sh`` for deleting a file
 * centralize variables definition to be used by scripts in one place:
    * ``gigadb/app/tools/dataset-backup-tool/config/variables``
  
The latter file is git-ignored and is created from example ``gigadb/app/tools/dataset-backup-tool/config/variables.example`` by copying it:
```
$ cd gigadb/app/tools/dataset-backup-tool
$ cp config/variables.example config/variables
```
The default values from the example should work for testing purpose.

#### Testing the variables setup

There is additional smoke test, part of the ``rclone-setup`` group, to check a ``config/variable`` file exists and is not setup 
with production bucket.

```
$ cd gigadb/app/tools/dataset-backup-tool
$ docker-compose run --rm backup_tool ./vendor/bin/codecept run -g rclone-setup tests/functional
```

#### Using the scripts

by default, --dry-run mode is active.

On a developer environment, we will use docker-compose. 
On a production environment (CNGB backup server), we will use the script directly for now.
In the examples below we use ``/app/scripts/sync_files.sh``, but it works exactly the same way for the other script
``/app/scripts/delete_files.sh``

```
$ cd gigadb/app/tools/dataset-backup-tool
$ docker-compose run --rm backup_tool /app/scripts/sync_files.sh
```

When confident the output shows what you want to happen, you can enable the verbose mode to proceed for real:
```
$ cd gigadb/app/tools/dataset-backup-tool
$ docker-compose run --rm backup_tool /app/scripts/sync_files.sh --verbose
```

>**Note 1:** using ``-v`` instead of ``--verbose`` is possible.

>**Note 2:** the deletion script is more interactive as it prompts the user for the file to delete and then ask for confirmation.

## Tools for GigaDB: To handle file permission issues [#684](https://github.com/gigascience/gigadb-website/issues/684)

In `cngb-gigadb-bak` server, to identify the permission of the files that is `not globally readable` in `/data/gigadb/pub/10.5524/` we could use:
```
$ find /data/gigadb/pub/10.5524/ ! -perm -g+r,u+r,o+r
```
To change the files to `globally readable`, we could use:
```
$ find /data/gigadb/pub/10.5524/ ! -perm -g+r,u+r,o+r -exec chmod a+r {} \;
```
The above is the main command to find not `globally readable` files recursively in a directory and fix it.

## Smoke tests for finding and fixing the permissions

### How to run the test:
Change directory to the `dataset-backup-tool`:
```
$ cd gigadb/app/tools/dataset-backup-tool
$ compose install
$ chmod a+x scripts/perm_to_ok.sh scripts/perm_to_not_ok.sh scripts/fix_permission.sh
```

There are 3 smoke tests in `tests/functional/FixPermissionCest.php` which would
identify and fix the permission in a mock directory `tests/_data/10.1234` :
* `listOkFilePermissions` will identify the permission of the `tests/_data/10.1234/100001_101009/100010/perm-ok.txt` which wasd created with `-rw-r--r--`.

* `listNotOkFilePermissions` will identify the permission of the `tests/_data/10.1234/100001_101009/100300/perm-not-ok.txt` which was created with `----------`.

* `changeNotOkFilePermissionToOk` will identify `non globally readable` files in `tests/_data/10.1234/` and change it to `globally readable` like this `-r--r--r--`.

To run these smoke tests:
```
$ docker-compose run --rm backup_tool ./vendor/bin/codecept run tests/functional/FixPermissionCest.php
```

###  Set up the `cronjob`:
The permission issues could occur regularly, so a regular fixing would be needed.  
To enable the `cronjob` which would start fixing permission at midnight of every day:
```
$ cd gigadb/app/tools/dataset-backup-tool
$ crontab < cronjob_fix_permission.txt
$ crontab -l 
0 0 * * * /app/scripts/fix-permissions.sh >> /tmp/permission_cron.log 2>&1
```









